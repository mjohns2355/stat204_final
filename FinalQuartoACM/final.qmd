---
title: "An Intentional AI for Hanabi: Reproduction"
format: pdf
author:
  - name: MJ Johns
  - name: Adam Khan

bibliography: references.bib
---

# Introduction

In this reproduction, we consider the paper "*An intentional AI for hanabi*" [@eger_intentional_2017] and propose the following research questions which reproduce and extend the findings of the prior paper:

-   RQ1: Does an intentional AI agent perform better in a collaborative game with a human than a baseline AI agent? (as measured by score)

-   RQ2: Is the playerâ€™s perception of the AI agent (e.g. perceived skill and/or enjoyment of playing together) correlated with higher score, regardless of implementation?

-   RQ3: Can we use Machine Learning to predict human responses to improve game-playing AI for more compatible cooperation?

All code for this project is publicly available on GitHub: <https://github.com/mjohns2355/stat204_final>

### Background and Motivation

Game-playing AI has proven to be a useful test-bed for exploring decision-making AI implementations, and has attracted a wide range of researchers and research interests [@yin_ai_2023; @hu_games_2024]. Similarly, AI within games offer a valuable opportunity to understand human perception of interacting, and collaborating, with AI [@laird_research_2002; @yannakakis_ai_2005]. As we rapidly approach a state where humans and AI interact and cooperate on a daily basis, it is becoming increasingly important to (1) develop AIs that meaningfully cooperate with humans, and (2) understand what aspects of human-AI interaction are enjoyable, challenging, or unpleasant for humans. With this paper, we aim to contribute to further understanding AI implementations and human-AI interaction through an analysis of an intentional AI for the game of hanabi. We used the following R packages: ggplot2, dplyr, patchwork, and vtable.

### Dataset Description

We are using two datasets for our analysis. First is the results of 240 games along with survey information from participants, which is publicly available on GitHub: <https://github.com/yawgmoth/HanabiData/> The second dataset is the turn-by-turn game logs from all games scoring over 20 points. While this data is not publicly available, we were granted access by one of the original authors. The game logs use one-hot-encoding to store the current game state (e.g. cards in each players' hand, cards that have been played or discarded, and what the current player knows from hints) and for the player's action on that turn (e.g. play card 1, discard card 4, etc.). The data is a matrix with dimensions 192731 x 393. The first 373 features represent Game State, the last 20 represent Player Action.

### Study Objectives

Our study has three objectives. First, to reproduce the results from the original paper using the extended dataset. Second, to extend the research to consider player perception of the AI (e.g. skill, likability). Lastly, to train a machine learning model on game logs to predict the player's next move from the current game state. This model could then be a proposed addition to the current AI implementation, offering a better collaborator with the human.

# Exploratory Data Analysis

We begin our reproduction by first exploring the data. The dataset consists of 240 rows/games and 14 columns, 10 of which being survey responses such as the player's perception of ai skill (1-5 scale) and enjoyment of playing with the ai (1-5 scale), while the others consist of game data, such as game score or the type of ai played with. We first checked for missing values and counted a total of 27 rows containing at least one missing value and thus decided to discard these rows for our subsequent analysis. Following this, we examined the distribution of scores, which can be seen in Figure 1. Inspecting this, we see that the distribution appears somewhat bimodal, with the most frequent score(s) being 17 and close to 17, and with distinct peaks at 5 and 9 as well. Table 1 shows a summary of the data.

```{r}
#| echo: false
#| warning: false
#| error: false

# add some libraries
library(ggplot2)
library(dplyr)
# install.packages("patchwork")
library(patchwork)

#MAKE PLOTS SIDE BY SIDE

# Load the survey ratings data
survey_response <- readr::read_csv('https://raw.githubusercontent.com/yawgmoth/HanabiData/refs/heads/master/participants.csv')

#dim(survey_response)
# count rows with NA entries
#sum(rowSums(is.na(survey_response)) > 0) #27 rows with missing data


# Preview the data
# summary(survey_response)

# cleanup and remove NAs
survey_complete <- survey_response[complete.cases(survey_response),]
survey_complete$ai <- survey_complete$ai %>% as.factor()
survey_complete$gamer <- survey_complete$gamer %>% as.factor()
survey_complete$age <- survey_complete$age %>% as.factor()
#dim(survey_complete)
survey_subset <- data.frame("Likability" = survey_complete$like,"Perceived Skill" = survey_complete$skill,"Perceived Intention" = survey_complete$intention,"Final Score" = survey_complete$score)
#summary(survey_subset)

#install.packages("vtable")

library(vtable)
sumtable(survey_subset)

# plot distribution of scores
plot1 <- ggplot(survey_complete, aes(x = score)) + geom_bar(bins = 26, fill = "steelblue") + labs(title = "Figure 1", subtitle = "Distribution of Scores", x = "Score", y = "Count")

# plot AI implementation vs score
plot2 <- ggplot(survey_complete, aes(x = ai, y = score, fill = ai)) +
  geom_boxplot() + labs(title = "Figure 2", subtitle = "AI Implementation Played With vs Game Score", x = "AI Implementation", y = "Game Score", fill = "AI")

plot1 + plot2

# plot percieved skill vs score
plot3 <- ggplot(survey_complete, aes(x = skill, y = score)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) + labs(title = "Figure 3", subtitle = "Perceived Skill vs Game Score", x = "Percieved Skill", y = "Game Score")


# aggregate score by perceived skill of AI
score_skill <- aggregate(score ~ skill, data = survey_complete, FUN = mean)
#score_skill

# aggregated score by whether they liked the AI
score_like <- aggregate(score ~ like, data = survey_complete, FUN = mean)
#score_like

# aggregate score by how intentional they thought the AI was
score_intention <- aggregate(score ~ intention, data = survey_complete, FUN = mean)
#score_intention

# aggregate score by which AI was used
score_ai <- aggregate(score ~ ai, data = survey_complete, FUN = mean)
#score_ai

# plot aggregated score by perceived skill
#plot(score_skill)
plot4 <- ggplot(score_skill, aes(x = skill, y = score)) +
  geom_point() +
  labs(title = "Figure 4", subtitle = "Average Perceived Skill by Game Score", x = "Percieved Skill", y = "Average Game Score")

# plot aggregated score by AI implementation
# plot(score_ai)

plot3 + plot4

```

# Methods

In the original study, participants were told they would be playing with an AI, but not which AI implementation they would be matched with. The original analysis involved an ANOVA comparing score based on AI implementation (Intentional, Outer, or Full). The original study's ANOVA resulted in (p\<0.0001) showing statistical significance for a relation between AI implementation and final score. To compare the specific implementations, the original study used a Tukey test to determine that there was a statistically significant difference between the intentional AI and the outer state AI (baseline), as well as between the outer state AI and the full AI, however they did not find statistical significance when comparing the intentional to the full AI.

Beyond reproducing the original ANOVA and Tukey tests using the larger dataset, we extend this research by considering the potential correlation of player perception of AI on the resulting score regardless of AI implementation. To measure this correlation, we use Spearman's Rank considering the 5-point Likert responses to perceived AI Skill and and how much the player liked their AI companion as potential variables. This method is used to test how strongly two sets of ranks are correlated, in this case we hypothesize that both perceived AI skill and likability will be correlated with higher game score.

Lastly, we used Machine Learning to train a model to predict player action based on current game state. Since the game log data was structured and labeled, we used XGBoost and a Neural Network to train two models and compare their performance.

# Results

To reproduce the original results, we conducted an ANOVA and Tukey Test. We verified the assumptions of the ANOVA with diagnostic plots. The Q-Q Residuals plot shows potential concern for skewness, however we have a large sample size and ANOVA is robust to mild normality violations.

```{r}
#| echo: false
#| warning: false
#| error: false

# anova with just AI

model <- aov(score ~ ai, survey_complete)
#summary(model)

# need AI and skill to be factors
survey_complete$ai <- factor(survey_complete$ai)
survey_complete$skillf <- factor(survey_complete$skill)

# anove with both AI and skill plus interaction
model2 <- aov(score ~ skillf + ai + skill:ai, survey_complete)
#summary(model2)

# anova with just skill (as factor)
model3 <- aov(score ~ skillf, survey_complete)
#summary(model3)

# anova with just AI (as factor)
model4 <- aov(score ~ ai, survey_complete)
summary(model4)

par(mfrow=c(2,2))
plot(model4)

# Tukey test compares all pairwise options from the ANOVA
tukey_results <- TukeyHSD(model4)
print(tukey_results) 

perceived_skill <- survey_complete$skill
final_score <- survey_complete$score

# Perform Spearman's rank correlation test
#result <- cor.test(perceived_skill, final_score, method = "spearman")

# Print the full result
#print(result)


```

Our results align with the findings of the original paper: There is a statistically significant difference favoring Intentional AI implementation scores compared ot the Outer or Full implementation scores to a 0.05 significance level. On the other hand, there is no statistically significant difference between the scores of the Outer and Full implementations as the original authors noted.

```{r}
#| echo: false
#| warning: false
#| error: false

perceived_skill <- survey_complete$skill
final_score <- survey_complete$score

# Perform Spearman's rank correlation test
result <- cor.test(perceived_skill, final_score, method = "spearman")

# Print the full result
print(result)

# run some diagnostics (monotonicity of data)
ggplot(survey_complete, aes(x = perceived_skill, y = final_score)) + 
  geom_point(color = "steelblue") + labs(title = "Perceived AI Skill vs Final Game Score", x = "Perceived AI Skill", y = "Final Score") + theme_minimal()+ geom_smooth(method = "lm", color = "orange")
#tapply(final_score, perceived_skill, median)
#tapply(final_score, perceived_skill, mean)


perceived_like <- survey_complete$like
final_score <- survey_complete$score

# Perform Spearman's rank correlation test
result <- cor.test(perceived_like, final_score, method = "spearman")

# Print the full result
print(result)
```

The Spearman's Rank Correlation shows that the correlation (rho = 0.1945) between Perceived Skill and Final Score is statistically significant to an alpha = 0.05 confidence level even with a rather small correlation coefficient. The test found no statistically significant correlation between Likability and Final Score to an alpha = 0.05, or even alpha = 0.1 confidence level.

**Machine Learning:** we used XGBoost and a Neural Network to train two potential models for predicting the next move based on current game state. We used 373 features, with One-Hot-Encoding to describe the current game state, at 20 output variables with One-Hot-Encoding representing the next move. The Nerual Network had a 41% success rate at correctly predicting the next move, while XGBoost had a 52% success rate. The code for both is available in the Appendix. One limitation of our approach is that we did not account for repeat players in the game logs, so one player who played many times may have had undue influence on the training data.

# Discussion & Conclusion

Based on our ANOVA and Tukey tests we are able to reproduce the results of the original study, and using the larger dataset provides evidence that the results are generalizable. This offers continued evidence to support the value of intentionality in AI design.

Based on our Spearman's Rank test, liking the AI companion was not statistically significantly correlated with higher score, however perceived AI skill was correlated with a higher score (p \< .005). This suggests that performing well in the game is not heavily reliant on the player *liking* their AI companion, and also that *liking* their companion is not heavily reliant on how well the AI is playing, however players who perceive the AI as more skillful were more likely to have high scoring games.

Lastly, our implementation of a Machine Learning model offers a proof-of-concept for future work that can integrate prediction into the intentional AI, allowing the AI to consider the likelihood of future moves based on the current game state.

# References